spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypassword
    driver-class-name: org.postgresql.Driver
  jpa:
    hibernate:
      ddl-auto: update
  application:
    name: pdf-analyzer
  servlet:
    multipart:
      enabled: true
      max-file-size: 50MB
      max-request-size: 50MB
  http:
    multipart:
      enabled: true
      max-file-size: 50MB
      max-request-size: 50MB
  ai:
    model:
      embedding: ollama
      chat: ollama
    ollama:
      init:
        pull-model-strategy: when_missing
      embedding:
        additional-models:
          - nomic-embed-text:latest
        options:
          model: nomic-embed-text:latest
      chat:
        additional-models:
          - gpt-oss:20b
        options:
          model: gpt-oss:20b
          num-ctx: 131027
          temperature: 0.2
    chat:
      memory:
        repository:
          jdbc:
            initialize-schema: always
      embedding:
    openai:
      api-key: ${OPENAI_API_KEY}
      embedding:
        options:
          model: text-embedding-3-small 				
    vectorstore:
      pgvector:
        initialize-schema: true
        index-type: HNSW
        distance-type: COSINE_DISTANCE
        dimensions: {VECTOR_DIMENSIONS:768}
        batching-strategy: TOKEN_COUNT
        max-document-batch-size: 10000
  sql:
    init:
      mode: always
server:
  tomcat:
    max-post-size: 52428800
    max-http-header-size: 65536
    max-swallow-size: 100MB
  shutdown: immediate

logging:
  level:
    org:
      apache:
        pdfbox:
          pgmodel:
            font:
              FileSystemFontProvider: ERROR

app:
  ai:
    topk: 50  # Number of document chunks to retrieve for RAG context
    maxChatHistory: 3  # Legacy - no longer used with new token-based memory
    maxChatTokens: 12000  # Maximum tokens for chat history (with summarization)
    recentMessageCount: 6  # Number of recent messages to keep in full (not summarized)
    beChatty: "no"

    # Adaptive Semantic Chunking Configuration
    # These values are hard-coded in AdaptiveSemanticChunker but documented here for reference
    # MIN_CHUNK_SIZE: 256 tokens - Minimum size to prevent tiny chunks
    # TARGET_CHUNK_SIZE: 384 tokens - Ideal size (~3-4 paragraphs or 1 section)
    # MAX_CHUNK_SIZE: 512 tokens - Hard upper limit (safe for nomic-embed-text model)
    # OVERLAP_SIZE: 100 tokens - Context overlap between adjacent chunks
    #
    # With topK=50 and target chunk size of 384 tokens:
    # Total retrieval context: ~19,200 tokens of semantically relevant content
    # Combined with 12,000 token chat history = ~31,200 total context
    # Well within the 131,027 token context window of gpt-oss:20b

    # Standard prompt template (without Chain-of-Thought)
    promptTemplate: |
      Use the following document excerpts to answer the user's question.

      DOCUMENT EXCERPTS:
      ---------------------
      <question_answer_context>
      ---------------------

      INSTRUCTIONS:
      1. The document excerpts above were automatically retrieved from uploaded documents
      2. Use this information to answer the user's question directly and naturally
      3. Do NOT ask the user to upload or provide documents - they are already available above
      4. Always cite your sources using the format: (Source: filename.pdf, page X)
      5. Only say you don't have information if the excerpts above are empty or completely irrelevant
      6. Answer in the same language as the question (Hebrew or English)

    # Chain-of-Thought prompt template (when enabled by user)
    promptTemplateWithCoT: |
      Use the following document excerpts to answer the user's question. Think step-by-step.

      DOCUMENT EXCERPTS:
      ---------------------
      <question_answer_context>
      ---------------------

      Think through your answer:

      **THINKING:**
      1. QUESTION: What is the user asking?
      2. CONTEXT: What relevant information is in the document excerpts above?
      3. CONFIDENCE:
         - HIGH: Clear evidence in multiple excerpts
         - MEDIUM: Partial information
         - LOW: Requires inference
         - NONE: No relevant information in excerpts

      **ANSWER:**
      [CONFIDENCE: HIGH/MEDIUM/LOW/NONE]

      [Provide your answer here]

      IMPORTANT:
      - Use the document excerpts provided above
      - Do NOT ask the user for documents - they are already provided
      - Cite sources: (Source: filename.pdf, page X)
      - Answer in the same language as the question

    systemText: |
      You are a helpful AI assistant for a document question-answering system.

      IMPORTANT: Documents have already been uploaded and indexed. When a user asks a question,
      relevant excerpts from these documents are automatically retrieved and provided to you
      in the context section below. You do NOT need the user to upload files - they are already available.

      Your role:
      1. Read the context information provided between the dashed lines
      2. Use this context to answer the user's questions about the documents
      3. Answer directly and naturally - do not say "Based on the context..." or ask for files
      4. Always cite your sources using the format: (Source: filename.pdf, page X)
      5. Only say you cannot answer if the provided context is truly empty or irrelevant
      6. Never make up information that is not in the provided context

      Language:
      - If asked in Hebrew, respond in Hebrew
      - If asked in English, respond in English

