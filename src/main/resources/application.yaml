spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypassword
    driver-class-name: org.postgresql.Driver
  jpa:
    hibernate:
      ddl-auto: update
  application:
    name: pdf-analyzer
  servlet:
    multipart:
      enabled: true
      max-file-size: 50MB
      max-request-size: 50MB
  http:
    multipart:
      enabled: true
      max-file-size: 50MB
      max-request-size: 50MB
  ai:
    model:
      embedding: ollama
      chat: ollama
    ollama:
      init:
        pull-model-strategy: when_missing
      embedding:
        additional-models:
          - nomic-embed-text:latest
        options:
          model: nomic-embed-text:latest
      chat:
        additional-models:
          - gpt-oss:20b
        options:
          model: gpt-oss:20b
          num-ctx: 131027
          temperature: 0.2
    chat:
      memory:
        repository:
          jdbc:
            initialize-schema: always
      embedding:
    openai:
      api-key: ${OPENAI_API_KEY}
      embedding:
        options:
          model: text-embedding-3-small 				
    vectorstore:
      pgvector:
        initialize-schema: true
        index-type: HNSW
        distance-type: COSINE_DISTANCE
        dimensions: {VECTOR_DIMENSIONS:768}
        batching-strategy: TOKEN_COUNT
        max-document-batch-size: 10000
  sql:
    init:
      mode: always
server:
  tomcat:
    max-post-size: 52428800
    max-http-header-size: 65536
    max-swallow-size: 100MB
  shutdown: immediate

logging:
  level:
    org:
      apache:
        pdfbox:
          pgmodel:
            font:
              FileSystemFontProvider: ERROR
      springframework:
        ai:
          chat:
            client:
              advisor: DEBUG
          vectorstore: DEBUG
    com:
      odedia:
        analyzer:
          services:
            DocumentAnalyzerService: DEBUG

app:
  ai:
    topk: 50  # Number of document chunks to retrieve for RAG context
    maxChatHistory: 3  # Legacy - no longer used with new token-based memory
    maxChatTokens: 12000  # Maximum tokens for chat history (with summarization)
    recentMessageCount: 6  # Number of recent messages to keep in full (not summarized)
    beChatty: "no"

    # Adaptive Semantic Chunking Configuration
    # These values are hard-coded in AdaptiveSemanticChunker but documented here for reference
    # MIN_CHUNK_SIZE: 256 tokens - Minimum size to prevent tiny chunks
    # TARGET_CHUNK_SIZE: 384 tokens - Ideal size (~3-4 paragraphs or 1 section)
    # MAX_CHUNK_SIZE: 512 tokens - Hard upper limit (safe for nomic-embed-text model)
    # OVERLAP_SIZE: 100 tokens - Context overlap between adjacent chunks
    #
    # With topK=50 and target chunk size of 384 tokens:
    # Total retrieval context: ~19,200 tokens of semantically relevant content
    # Combined with 12,000 token chat history = ~31,200 total context
    # Well within the 131,027 token context window of gpt-oss:20b

    # Standard prompt template (without Chain-of-Thought)
    # MUST include <query> and <question_answer_context> placeholders per Spring AI requirements
    promptTemplate: |
      Context information from uploaded documents:
      ---------------------
      <question_answer_context>
      ---------------------

      Question: <query>

      Instructions:
      - Answer the question using ONLY the context information above
      - The context was automatically retrieved - do NOT ask for documents
      - Be direct and natural in your response
      - Always cite sources: (Source: filename.pdf, page X)
      - If the context doesn't contain the answer, say you don't have that information
      - Answer in the same language as the question

    # Chain-of-Thought prompt template (when enabled by user)
    # MUST include <query> and <question_answer_context> placeholders per Spring AI requirements
    promptTemplateWithCoT: |
      Context information from uploaded documents:
      ---------------------
      <question_answer_context>
      ---------------------

      Question: <query>

      Think step-by-step before answering:

      **THINKING:**
      1. What is the user asking?
      2. What relevant information is in the context above?
      3. Confidence level:
         - HIGH: Clear evidence in multiple parts
         - MEDIUM: Partial information
         - LOW: Requires inference
         - NONE: No relevant information

      **ANSWER:**
      [CONFIDENCE: HIGH/MEDIUM/LOW/NONE]

      [Your answer here - cite sources]

      Instructions:
      - Answer using ONLY the context above
      - Do NOT ask for documents - they are already provided
      - Cite: (Source: filename.pdf, page X)
      - Answer in the same language as the question

    systemText: |
      You are a helpful AI assistant for a document question-answering system.

      IMPORTANT: Documents have already been uploaded and indexed. When a user asks a question,
      relevant excerpts from these documents are automatically retrieved and provided to you
      in the context section below. You do NOT need the user to upload files - they are already available.

      Your role:
      1. Read the context information provided between the dashed lines
      2. Use this context to answer the user's questions about the documents
      3. Answer directly and naturally - do not say "Based on the context..." or ask for files
      4. Always cite your sources using the format: (Source: filename.pdf, page X)
      5. Only say you cannot answer if the provided context is truly empty or irrelevant
      6. Never make up information that is not in the provided context

      Language:
      - If asked in Hebrew, respond in Hebrew
      - If asked in English, respond in English

