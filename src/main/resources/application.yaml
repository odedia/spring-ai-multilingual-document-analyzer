spring:
  datasource:
    url: jdbc:postgresql://localhost:5432/mydb
    username: myuser
    password: mypassword
    driver-class-name: org.postgresql.Driver
  jpa:
    hibernate:
      ddl-auto: update
  application:
    name: pdf-analyzer
  servlet:
    multipart:
      enabled: true
      max-file-size: 50MB
      max-request-size: 50MB
  http:
    multipart:
      enabled: true
      max-file-size: 50MB
      max-request-size: 50MB
  ai:
    model:
      embedding: ollama
      chat: ollama
    ollama:
      init:
        pull-model-strategy: when_missing
      embedding:
        additional-models:
          - nomic-embed-text:latest
        options:
          model: nomic-embed-text:latest
      chat:
        additional-models:
          - gpt-oss:20b
        options:
          model: gpt-oss:20b
          num-ctx: 131027
    chat:
      memory:
        repository:
          jdbc:
            initialize-schema: always
      embedding:
    openai:
      api-key: ${OPENAI_API_KEY}
      embedding:
        options:
          model: text-embedding-3-small 				
    vectorstore:
      pgvector:
        initialize-schema: true
        index-type: HNSW
        distance-type: COSINE_DISTANCE
        dimensions: {VECTOR_DIMENSIONS:768}
        batching-strategy: TOKEN_COUNT
        max-document-batch-size: 10000
  sql:
    init:
      mode: always
server:
  tomcat:
    max-post-size: 52428800
    max-http-header-size: 65536
    max-swallow-size: 100MB
  shutdown: immediate

logging:
  level:
    org:
      apache:
        pdfbox:
          pgmodel:
            font:
              FileSystemFontProvider: ERROR

app:
  ai:
    topk: 50  # Number of document chunks to retrieve for RAG context
    maxChatHistory: 3  # Legacy - no longer used with new token-based memory
    maxChatTokens: 12000  # Maximum tokens for chat history (with summarization)
    recentMessageCount: 6  # Number of recent messages to keep in full (not summarized)
    beChatty: "no"

    # Adaptive Semantic Chunking Configuration
    # These values are hard-coded in AdaptiveSemanticChunker but documented here for reference
    # MIN_CHUNK_SIZE: 256 tokens - Minimum size to prevent tiny chunks
    # TARGET_CHUNK_SIZE: 384 tokens - Ideal size (~3-4 paragraphs or 1 section)
    # MAX_CHUNK_SIZE: 512 tokens - Hard upper limit (safe for nomic-embed-text model)
    # OVERLAP_SIZE: 100 tokens - Context overlap between adjacent chunks
    #
    # With topK=50 and target chunk size of 384 tokens:
    # Total retrieval context: ~19,200 tokens of semantically relevant content
    # Combined with 12,000 token chat history = ~31,200 total context
    # Well within the 131,027 token context window of gpt-oss:20b

    # Standard prompt template (without Chain-of-Thought)
    promptTemplate: |
      <query>
      Context information is below:
      ---------------------
      <question_answer_context>
      ---------------------
      Answer the user's question directly and clearly using the context above.
      Cite your sources by referencing chunk numbers when possible.
      Format: [Your answer] (Source: chunk X)

      If information is incomplete or missing, acknowledge it honestly.

    # Chain-of-Thought prompt template (when enabled by user)
    promptTemplateWithCoT: |
      <query>
      Context information is below:
      ---------------------
      <question_answer_context>
      ---------------------

      Before answering, think through the question step-by-step using the THINKING section:

      **THINKING:**
      1. QUESTION ANALYSIS
         - What is the user really asking?
         - What type of question is this?

      2. CONTEXT REVIEW
         - Which chunks are most relevant?
         - Is the information sufficient?
         - Are there contradictions?

      3. CONFIDENCE ASSESSMENT
         - HIGH: Direct evidence in multiple chunks
         - MEDIUM: Partial information or single source
         - LOW: Weak evidence or requires inference

      **ANSWER:**
      [CONFIDENCE: HIGH/MEDIUM/LOW]

      [Your detailed answer with inline citations]
      (Source: chunk X, chunk Y)

      If uncertain or information is missing, state it clearly.

    systemText: |
      You are a friendly, helpful assistant that manages a document archive.

      Key guidelines:
      - Answer directly without meta-phrases like "Based on the context..."
      - Always cite sources using chunk numbers
      - Be honest about uncertainty
      - Never make up information not in the context
      - If asked in Hebrew, respond in Hebrew
      - If asked in English, respond in English

